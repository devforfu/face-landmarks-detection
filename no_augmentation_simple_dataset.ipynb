{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from utils import to_absolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(Path.home()/'data'/'keypoints'/'training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sz = 96\n",
    "np_shape = img_sz, img_sz\n",
    "torch_shape = 1, img_sz, img_sz\n",
    "num_of_landmarks = 30\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data.count().plot.bar(color='royalblue', figsize=(12, 8))\n",
    "_ = ax.set_xticklabels(ax.get_xticklabels(), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(record, coord):\n",
    "    return [v for k, v in record.items() if k.endswith(f'_{coord}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(record, shape):\n",
    "    return np.fromstring(record.Image, sep=' ').reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target(record):\n",
    "    xs, ys = [get(record, coord) for coord in ('x', 'y')]\n",
    "    return np.r_[xs, ys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(target):\n",
    "    return target[:num_of_landmarks//2], target[num_of_landmarks//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(df, i, ax=None, figsize=(4, 4)):\n",
    "    record = df.iloc[i]\n",
    "    sample = create_sample(record, np_shape)\n",
    "    target = create_target(record)\n",
    "    if ax is None:\n",
    "        f, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    ax.set_title(f'#{i}')\n",
    "    ax.imshow(sample, cmap='gray')\n",
    "    ax.scatter(*split(target), color='lightgreen', edgecolor='white', alpha=0.8)\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_grid(df, n=5, figsize=(10, 10), h_pad=0.05, w_pad=0.05):\n",
    "    f, axes = plt.subplots(n, n, figsize=figsize)\n",
    "    n_images = len(df)\n",
    "    indexes = np.random.choice(n_images, size=n*n, replace=False)\n",
    "    for idx, ax in zip(indexes, axes.flat):\n",
    "        show(df, idx, ax=ax)\n",
    "    f.tight_layout(h_pad=h_pad, w_pad=w_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_grid(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = data[data.columns[data.columns != 'Image']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = SimpleImputer().fit_transform(landmarks) \n",
    "imputed_data = pd.DataFrame(X, columns=landmarks.columns)\n",
    "imputed_data['Image'] = data.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_grid(imputed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from layers import conv, fc, res3x3, bottleneck, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(imputed_data.apply(lambda x: create_sample(x, torch_shape), axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(imputed_data.drop(columns='Image').apply(lambda x: create_target(x), axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(X, y, test_size=0.2):\n",
    "    X_norm = X/255\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    y_norm = scaler.fit_transform(y)\n",
    "    subsets = train_test_split(X_norm, y_norm, test_size=0.2, random_state=seed)\n",
    "    X_train, X_test, y_train, y_test = [\n",
    "        torch.tensor(subset, dtype=torch.float32) \n",
    "        for subset in subsets]\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    valid_ds = TensorDataset(X_test, y_test)\n",
    "    return train_ds, valid_ds, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(train_ds, valid_ds, bs=512, jobs=0):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs)\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer, schedule, last_epoch=-1):\n",
    "        assert callable(schedule)\n",
    "        self.schedule = schedule\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [self.schedule(self.last_epoch, lr) for lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(t_max, eta_min=0):\n",
    "    \n",
    "    def scheduler(epoch, base_lr):\n",
    "        t = epoch % t_max\n",
    "        return eta_min + (base_lr - eta_min)*(1 + math.cos(math.pi*t/t_max))/2\n",
    "    \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, ni, no):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(ni, 16, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(128, no)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x.view(x.size(0), -1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, ni, no):\n",
    "        super().__init__()\n",
    "        layers  = conv(ni,  16, kernel=3, stride=2, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += conv(16,  16, kernel=3, stride=1, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += conv(16,  32, kernel=3, stride=2, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += conv(32,  32, kernel=3, stride=1, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += conv(32,  64, kernel=3, stride=2, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += conv(64,  64, kernel=3, stride=1, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += conv(64, 128, kernel=3, stride=2, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += bottleneck()\n",
    "        layers += fc(256, 128, bn=True, activ='relu')\n",
    "        layers += fc(128, no, bn=False)\n",
    "        self.model = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, ni, no):\n",
    "        super().__init__()\n",
    "        layers = conv(ni, 32, kernel=3, stride=2, pad=1, activ='leaky_relu', bn=True)\n",
    "        layers += [\n",
    "            res3x3(3,  32,  32, activ='leaky_relu'),\n",
    "            res3x3(3,  32,  64, activ='leaky_relu', upsample=True),\n",
    "            res3x3(3,  64,  64, activ='leaky_relu'),\n",
    "            res3x3(3,  64, 128, activ='leaky_relu', upsample=True),\n",
    "            res3x3(3, 128, 128, activ='leaky_relu')]\n",
    "        layers += bottleneck()\n",
    "        layers += fc(256, 128, bn=True, activ='relu')\n",
    "        layers += fc(128, no, bn=False)\n",
    "        self.model = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.model:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds, val_ds, scaler = create_datasets(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-2\n",
    "# wd = 1e-5\n",
    "# bs = 800\n",
    "# n_epochs = 100\n",
    "# patience = 20\n",
    "# no_improvements = 0\n",
    "# jobs = 12\n",
    "# best_loss = np.inf\n",
    "# best_weights = None\n",
    "# history = []\n",
    "# lr_history = []\n",
    "\n",
    "# trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs, jobs=jobs)\n",
    "# dataset_sizes = {'train': len(trn_ds), 'val': len(val_ds)}\n",
    "\n",
    "# # net = SimpleNet(1, num_of_landmarks)\n",
    "# # net = ConvNet(1, num_of_landmarks)\n",
    "# net = ResNet(1, num_of_landmarks)\n",
    "# net.to(device)\n",
    "# criterion = nn.MSELoss(reduction='sum')\n",
    "# optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n",
    "# iterations_per_epoch = len(trn_dl)\n",
    "# scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#     stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "#     for phase, loader in (('train', trn_dl), ('val', val_dl)):\n",
    "#         training = phase == 'train'\n",
    "#         running_loss = 0.0\n",
    "#         n_batches = 0\n",
    "        \n",
    "#         for batch in loader:\n",
    "#             x_batch, y_batch = [b.to(device) for b in batch]\n",
    "#             optimizer.zero_grad()\n",
    "        \n",
    "#             # compute gradients only during 'train' phase\n",
    "#             with torch.set_grad_enabled(training):\n",
    "#                 outputs = net(x_batch)\n",
    "#                 loss = criterion(outputs, y_batch)\n",
    "                \n",
    "#                 # don't update weights and rates when in 'val' phase\n",
    "#                 if training:\n",
    "#                     scheduler.step()\n",
    "#                     loss.backward()\n",
    "#                     optimizer.step()\n",
    "#                     lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "#             running_loss += loss.item()\n",
    "            \n",
    "#         epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#         stats[phase] = epoch_loss\n",
    "        \n",
    "#         # early stopping: save weights of the best model so far\n",
    "#         if phase == 'val':\n",
    "#             if epoch_loss < best_loss:\n",
    "#                 print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "#                 best_loss = epoch_loss\n",
    "#                 best_weights = copy.deepcopy(net.state_dict())\n",
    "#                 torch.save(best_weights, 'best_weights.pth')\n",
    "#                 no_improvements = 0\n",
    "#             else:\n",
    "#                 no_improvements += 1\n",
    "                \n",
    "#     history.append(stats)\n",
    "#     print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "#     if no_improvements >= patience:\n",
    "#         print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "#         break\n",
    "\n",
    "# if best_weights is not None:\n",
    "#     print(f'Loading the best weights with the training loss: {best_loss:.4f}')\n",
    "#     net.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ResNet(1, num_of_landmarks).to(device)\n",
    "net.load_state_dict(torch.load('best_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_clone(t):\n",
    "    return t.clone().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataset, i, img_sz=img_sz, device=device, scaler=scaler):\n",
    "    test_img, test_pts = dataset[i]\n",
    "    model.train(False)\n",
    "    [pred] = model(test_img[None].to(device))\n",
    "    model.train(True)\n",
    "    np_pts = np_clone(pred)\n",
    "    np_pts = scaler.inverse_transform(np_pts.reshape(1, -1)).flatten()\n",
    "    np_img = np_clone(test_img).transpose(1, 2, 0).reshape(96, 96)\n",
    "    np_img *= 255\n",
    "    np_img = np_img.astype(np.uint8)\n",
    "    rescaled_test_pts = scaler.inverse_transform(test_pts.reshape(1, -1)).flatten()\n",
    "    return np_pts, (np_img, rescaled_test_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model, dataset, n, figsize=(10, 10)):\n",
    "    f, axes = plt.subplots(n, n, figsize=figsize)\n",
    "    n_samples = len(dataset)\n",
    "    indexes = np.random.choice(n_samples, size=n*n, replace=False)\n",
    "    for ax, idx in zip(axes.flat, indexes):\n",
    "        pred_pts, (test_img, test_pts) = predict(model, dataset, idx)\n",
    "        ax.imshow(test_img, cmap='gray')\n",
    "        ax.scatter(*split(test_pts), color='lightgreen', edgecolor='white', s=50, alpha=0.8, \n",
    "                   label='gt')\n",
    "        ax.scatter(*split(pred_pts), color='red', marker='x', s=20, label='predicted')\n",
    "        ax.set_axis_off()\n",
    "    handles, labels = axes.flat[0].get_legend_handles_labels()\n",
    "    f.legend(handles, labels, loc='upper center', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(net, val_ds, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patch(x, y, w, h, image):\n",
    "    c = image.size(0)\n",
    "    patch = torch.zeros((c, h, w), dtype=image.dtype)\n",
    "    for q in range(h):\n",
    "        for p in range(w):\n",
    "            yq = y + q - (h - 1)/2\n",
    "            xp = x + p - (w - 1)/2\n",
    "            xd = 1 - (xp - math.floor(xp))\n",
    "            xu = 1 - (math.ceil(xp) - xp)\n",
    "            yd = 1 - (yq - math.floor(yq))\n",
    "            yu = 1 - (math.ceil(yq) - yq)\n",
    "            for idx in range(c):\n",
    "                patch[idx, q, p] = (\n",
    "                    image[idx, math.floor(yq), math.floor(xp)]*yd*xd + \n",
    "                    image[idx, math.floor(yq),  math.ceil(xp)]*yd*xu +\n",
    "                    image[idx,  math.ceil(yq), math.floor(xp)]*yu*xd +\n",
    "                    image[idx,  math.ceil(yq),  math.ceil(xp)]*yu*xu\n",
    "                ).item()\n",
    "    return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches(image, points, n=None, sz=31):\n",
    "    if n is None:\n",
    "        n = len(points)//2\n",
    "    patches = []\n",
    "    for i in range(n):\n",
    "        x_val, y_val = points[i], points[i + n]\n",
    "        patch = generate_patch(x_val, y_val, sz, sz, image)\n",
    "        patches.append(patch)\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches_faster(image, points, n=None, sz=16):\n",
    "    if n is None:\n",
    "        n = len(points)//2\n",
    "    patches = torch.zeros((n, 1, sz, sz))\n",
    "    h, w = image.shape[-2:]\n",
    "    for i in range(n):\n",
    "        x_val, y_val = int(points[i]), int(points[i + n])\n",
    "        patches[i] = image[:, y_val-sz//2:y_val+sz//2, x_val-sz//2:x_val+sz//2]\n",
    "    interp = F.interpolate(patches, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "    return interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensors(images, cols=4, figsize=(10, 10)):\n",
    "    n = len(images)\n",
    "    rows = int(math.ceil(n/cols))\n",
    "    f, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    for ax in axes.flat:\n",
    "        ax.set_axis_off()\n",
    "    for ax, image in zip(axes.flat, images):\n",
    "        image = np_clone(image).transpose(1, 2, 0).squeeze()\n",
    "        ax.imshow(image, cmap='gray' if len(image.shape) == 2 else None)\n",
    "        ax.set_title(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, pts = torch.tensor(X[0]), y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches = generate_patches(img, pts, sz=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = generate_patches_faster(img, pts, sz=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tensors(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_patches(image, points, sz=16):\n",
    "#     n = len(points)//2\n",
    "#     h, w = image.shape[-2:]\n",
    "#     xs, ys = to_absolute(points[:n], points[n:], w, h)\n",
    "#     patches = []\n",
    "#     for i in range(n):\n",
    "#         x, y = int(xs[i]), int(ys[i])\n",
    "#         patch = image[:, y-sz//2:y+sz//2, x-sz//2:x+sz//2]\n",
    "#         patches.append(patch)\n",
    "#     return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualModel(nn.Module):\n",
    "    def __init__(self, n_landmarks, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # should this be (n x 15), stacked patches?\n",
    "        layers  = conv(n_landmarks//2, 16, 6, stride=2, bn=True, activ='relu')\n",
    "        layers += conv(16, 16, 3, stride=1, bn=True, activ='relu')\n",
    "        layers += [Flatten()]\n",
    "        layers += fc(256, 128, bn=True, activ='relu')\n",
    "        layers += fc(128, n_landmarks, bn=False)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, s, x):\n",
    "        s.clamp_(-0.95, 0.95)\n",
    "        # it was 4 originally, probably worth to do less scaling\n",
    "        # x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        stacked = []\n",
    "        for t1, t2 in zip(x, s):\n",
    "            # patches = [p for p in generate_patches_faster(t1, t2, sz=self.patch_size)]\n",
    "            patches = generate_patches_faster(t1, t2, sz=self.patch_size)\n",
    "            patches.squeeze_()\n",
    "            stacked.append(patches)\n",
    "        t = torch.cat(stacked, dim=0)\n",
    "        for layer in self.layers:\n",
    "            t = layer(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksRegressor(nn.Module):\n",
    "    def __init__(self, n_landmarks, s0_model, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.s0_model = s0_model\n",
    "        self.r1 = ResidualModel(n_landmarks, patch_size)\n",
    "        self.r2 = ResidualModel(n_landmarks, patch_size)\n",
    "        self.r3 = ResidualModel(n_landmarks, patch_size)\n",
    "        self.s0_model.train(False)\n",
    "    \n",
    "    def forward(self, face_image):\n",
    "        s0 = self.s0_model(face_image)\n",
    "        s0.add_(self.r1(s0, face_image))\n",
    "        s0.add_(self.r2(s0, face_image))\n",
    "        s0.add_(self.r3(s0, face_image))\n",
    "        return s0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-5\n",
    "bs = 800\n",
    "n_epochs = 100\n",
    "patience = 20\n",
    "no_improvements = 0\n",
    "jobs = 12\n",
    "best_loss = np.inf\n",
    "best_weights = None\n",
    "history = []\n",
    "lr_history = []\n",
    "\n",
    "trn_ds, val_ds, scaler = create_datasets(X, y)\n",
    "trn_dl, val_dl = create_loaders(trn_ds, val_ds, bs, jobs=jobs)\n",
    "dataset_sizes = {'train': len(trn_ds), 'val': len(val_ds)}\n",
    "\n",
    "reg = LandmarksRegressor(num_of_landmarks, net).to(device)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = optim.Adam(reg.parameters(), lr=lr, weight_decay=wd)\n",
    "iterations_per_epoch = len(trn_dl)\n",
    "scheduler = CyclicLR(optimizer, cosine(t_max=iterations_per_epoch * 2, eta_min=lr/100))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    stats = {'epoch': epoch + 1, 'total': n_epochs}\n",
    "    \n",
    "    for phase, loader in (('train', trn_dl), ('val', val_dl)):\n",
    "        training = phase == 'train'\n",
    "        running_loss = 0.0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for batch in loader:\n",
    "            x_batch, y_batch = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute gradients only during 'train' phase\n",
    "            with torch.set_grad_enabled(training):\n",
    "                outputs = reg(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                # don't update weights and rates when in 'val' phase\n",
    "                if training:\n",
    "                    scheduler.step()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    lr_history.extend(scheduler.get_lr())\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        stats[phase] = epoch_loss\n",
    "        \n",
    "        # early stopping: save weights of the best model so far\n",
    "        if phase == 'val':\n",
    "            if epoch_loss < best_loss:\n",
    "                print('loss improvement on epoch: %d' % (epoch + 1))\n",
    "                best_loss = epoch_loss\n",
    "                best_weights = copy.deepcopy(reg.state_dict())\n",
    "                torch.save(best_weights, 'best_reg_weights.pth')\n",
    "                no_improvements = 0\n",
    "            else:\n",
    "                no_improvements += 1\n",
    "                \n",
    "    history.append(stats)\n",
    "    print('[{epoch:03d}/{total:03d}] train: {train:.4f} - val: {val:.4f}'.format(**stats))\n",
    "    if no_improvements >= patience:\n",
    "        print('early stopping after epoch {epoch:03d}'.format(**stats))\n",
    "        break\n",
    "\n",
    "if best_weights is not None:\n",
    "    print(f'Loading the best weights with the training loss: {best_loss:.4f}')\n",
    "    reg.load_state_dict(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_predictions(reg, val_ds, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
